{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04130f62",
   "metadata": {},
   "source": [
    "# Introduction to Satellite Imagery Preprocessing and Model Training and Prediction\n",
    "\n",
    "This notebook provides a comprehensive workflow for processing satellite imagery and predicting water bodies using a combination of remote sensing data and exogenous factors. The model was trained on a project area in the riverbed of the Isar in the Alps. The main objectives of this notebook are:\n",
    "\n",
    "1. **Satellite Imagery Retrieval and Preprocessing**:\n",
    "   - Retrieve Sentinel-2 satellite imagery from Google Earth Engine (GEE).\n",
    "   - Calculate the Normalized Difference Water Index (NDWI) to identify water bodies.\n",
    "   - Mask snow/ice regions to improve the accuracy of water detection.\n",
    "\n",
    "2. **Water Mask Creation**:\n",
    "   - Generate binary water masks based on individual NDWI thresholds.\n",
    "   - Save the processed water masks as GeoTIFF files for further analysis.\n",
    "\n",
    "3. **Prepare the Input Data for Model Training and the Integration of Exogenous Factors**:\n",
    "   - Load exogenous factors (e.g., precipitation, discharge) from external datasets.\n",
    "   - Combine these factors with satellite imagery to enhance predictive modeling.\n",
    "\n",
    "4. **Model Building and Training**:\n",
    "   - Train a CNN-LSTM model to predict future water body extents based on historical satellite imagery and exogenous factors. \n",
    "   - Evaluate the model's performance using metrics such as Mean Squared Error (MSE) and Intersection over Union (IoU).\n",
    "\n",
    "5. **Visualization**:\n",
    "   - Visualize the predicted water masks and compare them with the ground truth.\n",
    "   - Analyze the model's accuracy and identify areas for improvement.\n",
    "\n",
    "This notebook serves as a step-by-step guide for researchers and practitioners working on hydrological modeling, water resource management, or environmental monitoring. By combining satellite imagery with machine learning techniques, it demonstrates how to derive actionable insights from remote sensing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import ee\n",
    "import geemap\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rasterio\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a83b0a",
   "metadata": {},
   "source": [
    "## Step 1: Satellite Imagery Retrieval and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7103cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Get Satellite Imagery from Google Earth Engine\n",
    "\n",
    "\n",
    "# Initialize Earth Engine API\n",
    "# Make sure to authenticate if you haven't done so already\n",
    "#ee.Authenticate()  # Uncomment this line if you need to authenticate\n",
    "# Initialize the Earth Engine library\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "\n",
    "#Define the coordinates of the bounding box (xmin, ymin) and (xmax, ymax) of your project area\n",
    "#Coordinates are in EPSG:4326\n",
    "#Example coordinates for a bounding box around Vorderri√ü, Germany\n",
    "bbox_coords = ((11.437948183612978, 47.55891195678332), (11.484581611702358, 47.56320074444652))\n",
    "geometry = ee.Geometry.Polygon([\n",
    "    [\n",
    "        [bbox_coords[0][0], bbox_coords[0][1]],  # (xmin, ymin)\n",
    "        [bbox_coords[0][0], bbox_coords[1][1]],  # (xmin, ymax)\n",
    "        [bbox_coords[1][0], bbox_coords[1][1]],  # (xmax, ymax)\n",
    "        [bbox_coords[1][0], bbox_coords[0][1]],  # (xmax, ymin)\n",
    "        [bbox_coords[0][0], bbox_coords[0][1]],  # Closing the polygon\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Define a function to calculate snow fraction of each image. Snow has similiar reflectance properties as water.\n",
    "def add_snow_fraction(image):\n",
    "    scl = image.select(\"SCL\")\n",
    "    snow_mask = scl.eq(11)  # SCL = 11 means snow/ice\n",
    "\n",
    "    snow_fraction = snow_mask.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=geometry,\n",
    "        scale=20,\n",
    "        maxPixels=1e8\n",
    "    ).get('SCL')  # result is a number between 0 and 1\n",
    "\n",
    "    return image.set('snow_fraction', snow_fraction)\n",
    "\n",
    "\n",
    "# Define NDWI calculation function (normalized difference using B3 and B8)\n",
    "def calculateNDWI(image):\n",
    "    ndwi = image.normalizedDifference([\"B3\", \"B8\"]).rename(\"NDWI\")\n",
    "    return image.addBands(ndwi)\n",
    "\n",
    "# Define a function to clip the image to your geometry\n",
    "def clip_image(image):\n",
    "    return image.clip(geometry)\n",
    "\n",
    "# Create a function to assign a chronological image_id based on the collection index\n",
    "def create_feature_with_index(image, index):\n",
    "    # Convert the index to a string to be used as the image ID\n",
    "    image_id = ee.Number(index)  # Using index as the image ID\n",
    "    timestamp = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')  # Format timestamp as YYYY-MM-DD\n",
    "    \n",
    "    # Create a feature with the image ID and timestamp\n",
    "    feature = ee.Feature(None, {\n",
    "        'image_id': image_id,\n",
    "        'timestamp': timestamp\n",
    "    })\n",
    "    return feature\n",
    "\n",
    "# Function to assign a chronological index to each image in the collection\n",
    "def add_index_to_collection(image_collection):\n",
    "    # Create a list of features with image_id as index\n",
    "    def assign_index(image, index):\n",
    "        return create_feature_with_index(image, index)\n",
    "    \n",
    "    # Map the function over the collection and add indices\n",
    "    feature_collection = image_collection.map(lambda image: assign_index(image, image_collection.toList(image_collection.size()).indexOf(image)))\n",
    "    return feature_collection\n",
    "\n",
    "\n",
    "# Step 1. 1: Load Sentinel-2 image collection. Filter by date, bounds, and cloud cover.\n",
    "sentinel = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "    .filterDate('2015-06-27', '2025-03-31') \\\n",
    "    .filterBounds(geometry) \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))  \n",
    "\n",
    "# Add snow fraction to each image\n",
    "sentinel_snow_filtered = sentinel.map(add_snow_fraction)\n",
    "\n",
    "# Filter images with < 3 % snow\n",
    "sentinel_clean = sentinel_snow_filtered.filter(ee.Filter.lt('snow_fraction', 0.03))\n",
    "\n",
    "processed_images = sentinel_clean.map(clip_image).map(calculateNDWI)\n",
    "\n",
    "\n",
    "# Map the function over the processed image collection to create a FeatureCollection with chronological image_ids\n",
    "features = add_index_to_collection(processed_images)\n",
    "\n",
    "# Step 1. 2: Export images and metadata to Google Drive. If you authenticated with your Google account in Google Earth Engine, the images will be saved in your Google Drive.\n",
    "\n",
    "# Export the FeatureCollection as a CSV to Google Drive\n",
    "export_task = ee.batch.Export.table.toDrive(\n",
    "    collection=features,\n",
    "    description='Image_Metadata',\n",
    "    fileFormat='CSV',\n",
    "    folder='Satellite_metadata',  # Folder in your Google Drive\n",
    "    fileNamePrefix='satellite_metadata_all',\n",
    ")\n",
    "\n",
    "# Start the export task\n",
    "export_task.start()\n",
    "\n",
    "print(\"Export task for image metadata started. Monitor Earth Engine for task completion.\")\n",
    "\n",
    "\n",
    "# Export each image in the collection\n",
    "def export_image(image, idx):  \n",
    "    # Export the image to a GeoTIFF using Earth Engine's export method\n",
    "    export_task = ee.batch.Export.image.toDrive(\n",
    "        image=image.select(\"NDWI\"),\n",
    "        description=None,\n",
    "        folder=\"GEE_Images_all\",\n",
    "        fileNamePrefix=f\"NDWI_{idx}\",\n",
    "        region=geometry,\n",
    "        scale=10,\n",
    "        crs='EPSG:4326',\n",
    "        maxPixels=1e8  # Increase the max pixels if necessary\n",
    "    )\n",
    "    export_task.start()  # Start the export task\n",
    "    print(f\"Exporting {idx}\")\n",
    "\n",
    "# Convert the image collection to a list of images. This triggers the actual fetching of the collection.\n",
    "image_list = processed_images.toList(processed_images.size())\n",
    "\n",
    "# Loop through the image collection and export each image\n",
    "for idx in range(image_list.size().getInfo()):\n",
    "    ee_image = ee.Image(image_list.get(idx))  # Get the image by its index in the list\n",
    "    export_image(ee_image, idx)\n",
    "\n",
    "print(\"Export tasks started. Monitor Earth Engine for task completion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad871a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 3: Add an index column to the CSV file exported from Earth Engine\n",
    "\n",
    "metadata_path = ''  \n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "metadata_df['Index'] = range(len(metadata_df)) #Creating an index column with ascending values\n",
    "\n",
    "output_path = ''  # Save the modified DataFrame to a new CSV file\n",
    "metadata_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Added new column 'Index' and saved it in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c26cf",
   "metadata": {},
   "source": [
    "## Step 2: Water Mask Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c34f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the ID from the filename\n",
    "# This function assumes the filename format is \"NDWI_<ID>.tif\"\n",
    "def extract_id_from_filename(filename):\n",
    "    return int(filename.split('_')[1].split('.')[0]) \n",
    "\n",
    "# Define a function to get the timestamp from the image ID and double check if the ID is in the new CSV file \n",
    "def get_timestamp_from_id(image_id):\n",
    "    timestamp_data = pd.read_csv('')  \n",
    "    row = timestamp_data[timestamp_data['Index'] == image_id]\n",
    "    if not row.empty:\n",
    "        return row['timestamp'].values[0]\n",
    "    else:\n",
    "        return None  # If the ID is not found, return None\n",
    "\n",
    "# Define a function to process NDWI images and create water masks\n",
    "def process_images_and_create_water_masks(folder_path):\n",
    "    output_dir_water_masks = '/'  # folder for saving water masks\n",
    "\n",
    "    # Make sure that the folder exists\n",
    "    if not os.path.exists(output_dir_water_masks):\n",
    "        os.makedirs(output_dir_water_masks)\n",
    "\n",
    "    # Order by numeric ID\n",
    "    image_files =[f for f in os.listdir(folder_path) if f.lower().endswith(('.tif', '.tiff'))]\n",
    "    image_files.sort(key=extract_id_from_filename)  \n",
    "\n",
    "    # Loop through each image file\n",
    "    for filename in image_files:\n",
    "        if filename.lower().endswith(('.tif', '.tiff')):\n",
    "            image_id = extract_id_from_filename(filename)\n",
    "            timestamp = get_timestamp_from_id(image_id)\n",
    "            if timestamp is None:\n",
    "                print(f\"No timestamp for {filename}. Pass.\")\n",
    "                continue\n",
    "            \n",
    "            # Open the NDWI image using GDAL\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            dataset = gdal.Open(file_path)\n",
    "            if dataset is None:\n",
    "                print(f\"There was a mistake while opening: {filename}.\")\n",
    "                continue\n",
    "            \n",
    "            # Read the NDWI band and calculate the individual threshold. \n",
    "            # For the project area possible fraction of water in the image is between 5,8 % and 6,5 %. \n",
    "            # Assuming a unique threshold for each image is not recommended.\n",
    "            band = dataset.GetRasterBand(1)\n",
    "            ndwi = band.ReadAsArray()\n",
    "            \n",
    "            threshold = np.percentile(ndwi, 93)\n",
    "            if np.isnan(threshold):\n",
    "                threshold = 0.05\n",
    "                print(f\"Threshold for {filename} is NaN. Set to {threshold}.\")\n",
    "            # Check if the threshold is within the expected range. Unless change it.\n",
    "            elif threshold < -0.1 or threshold > 0.1:\n",
    "                # Get the sign of the threshold (-1, +1)\n",
    "                sign = np.sign(threshold) \n",
    "                \n",
    "                # If the threshold is negative, set it to -0.1, but proportional to the size of the threshold\n",
    "                if threshold < -0.1:\n",
    "                    threshold = -0.1 + (threshold + 0.1) * 0.7\n",
    "                # If the threshold is positive, set it to 0.1, but proportional to the size of the threshold\n",
    "                elif threshold > 0.1:\n",
    "                    threshold = 0.1 - (threshold - 0.1) * 0.7\n",
    "\n",
    "                threshold = np.clip(threshold, -0.1, 0.1)\n",
    "                \n",
    "                print(f\"Invalid threshold for {filename}: Set to {threshold}.\")\n",
    "            else:\n",
    "                print(f\"Valid threshold for {filename} is {threshold}.\")\n",
    "\n",
    "            # Create a binary water mask based on the threshold and save it as a new GeoTIFF \n",
    "            water_mask = ndwi > threshold\n",
    "        \n",
    "            water_mask_filename = os.path.join(output_dir_water_masks, f\"water_mask_{os.path.splitext(filename)[0]}.tif\")\n",
    "            driver = gdal.GetDriverByName('GTiff')\n",
    "            mask_dataset = driver.Create(\n",
    "                water_mask_filename,\n",
    "                dataset.RasterXSize,\n",
    "                dataset.RasterYSize,\n",
    "                1,\n",
    "                gdal.GDT_Byte\n",
    "            )\n",
    "            mask_dataset.SetGeoTransform(dataset.GetGeoTransform())\n",
    "            mask_dataset.SetProjection(dataset.GetProjection())\n",
    "            mask_band = mask_dataset.GetRasterBand(1)\n",
    "            mask_band.WriteArray(water_mask.astype(np.uint8)) \n",
    "            mask_dataset = None\n",
    "            print(f\"Saved water mask: {water_mask_filename}\")\n",
    "\n",
    "    print(\"All water masks created and saved.\")\n",
    "    return\n",
    "\n",
    "\n",
    "folder_path = ''  # Path to the folder with NDWI images\n",
    "\n",
    "# Call the function to process images and create water masks\n",
    "process_images_and_create_water_masks(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8f4d6",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Train Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e04891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. 1: Load water masks from the folder and add a new dimension to the images because the model requires 3D input or 4D input\n",
    "\n",
    "def extract_id_from_filename(filename):\n",
    "    return int(filename.split('_')[-1].split('.')[0]) \n",
    "\n",
    "# Define a function to load all TIF images from a directory in sorted order\n",
    "def load_all_tif_images(directory):\n",
    "    image_files = [f for f in os.listdir(directory) if f.lower().endswith('.tif')]\n",
    "    image_files.sort(key=extract_id_from_filename)\n",
    "    \n",
    "    images = []\n",
    "    for filename in image_files:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            image = src.read(1)  \n",
    "            image = np.expand_dims(image, axis=-1) # Add a new dimension\n",
    "            images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "images_directory = \"/\" # Path to the folder with water masks\n",
    "images = load_all_tif_images(images_directory)\n",
    "\n",
    "print(f\"Amount of loaded images: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3. 2: Prepare exogenous factors for train data set\n",
    "\n",
    "exo_factors_df = pd.read_csv(\"\")  # Path to the CSV file with exogenous factors\n",
    "\n",
    "# Convert the 'timestamp' column to datetime format\n",
    "exo_factors_df['timestamp'] = pd.to_datetime(exo_factors_df['timestamp'])\n",
    "\n",
    "# Calculate the time difference in days between consecutive timestamps\n",
    "exo_factors_df['time_diff_days'] = exo_factors_df['timestamp'].diff().dt.days\n",
    "\n",
    "# Extract the month from the timestamp and one-hot encode it and save the columns names\n",
    "exo_factors_df['month'] = exo_factors_df['timestamp'].dt.month\n",
    "month_one_hot = pd.get_dummies(exo_factors_df['month'], prefix='month')\n",
    "month_one_hot_columns = list(month_one_hot.columns)\n",
    "\n",
    "# Add the one-hot encoded month columns to the DataFrame\n",
    "exo_factors_df = pd.concat([exo_factors_df, month_one_hot], axis=1)\n",
    "\n",
    "# Drop the original month column\n",
    "exo_factors_df.drop(columns=['month'], inplace=True)\n",
    "\n",
    "# Convert other important columns of the DataFrame. Save only the columns that are needed for the model.\n",
    "exo_factors_df['discharge'] = pd.to_numeric(exo_factors_df['delta_discharge'], errors='coerce')\n",
    "exo_factors_df['precipitation'] = pd.to_numeric(exo_factors_df['delta_precipitation'], errors='coerce')\n",
    "exo_factors_df['prec_extreme'] = pd.to_numeric(exo_factors_df['extreme_precipitation'], errors='coerce')\n",
    "exo_factors_df['disc_extreme'] = pd.to_numeric(exo_factors_df['extreme_discharge'], errors='coerce')\n",
    "\n",
    "exo_factors_df = exo_factors_df[['timestamp', 'image_id', 'discharge', 'precipitation', 'prec_extreme', 'disc_extreme', 'time_diff_days'] + list(month_one_hot.columns)]\n",
    "exo_factors_df.fillna(0, inplace=True)\n",
    "print(exo_factors_df.head())\n",
    "\n",
    "# Convert the DataFrame to a NumPy array because the model requires 3D input or 4D input\n",
    "exo_factors = exo_factors_df[['discharge', 'precipitation', 'prec_extreme', 'disc_extreme', 'time_diff_days'] + list(month_one_hot_columns)].values\n",
    "print(exo_factors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. 3: Split the data into training, validation, and test sets (e. g. 70% train, 15% val, 15% test)\n",
    "\n",
    "train_images = images[:173]  \n",
    "train_exo = exo_factors[:173]  \n",
    "\n",
    "val_images = images[173:210]  \n",
    "val_exo = exo_factors[173:210]\n",
    "\n",
    "test_images = images[210:]  \n",
    "test_exo = exo_factors[210:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c347db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3. 4: Create a data generator for the training set\n",
    "# This generator will yield batches of images and exogenous factors for training\n",
    "\n",
    "def data_generator_with_future_exo(images, exo_factors, batch_size, time_steps=5):\n",
    "    while True:  \n",
    "        indices = np.random.choice(len(images) - time_steps - 1, batch_size)\n",
    "\n",
    "        X_images_batch = []  \n",
    "        exogenous_batch = []  \n",
    "        future_exo_batch = []  \n",
    "        Y_image_batch = []  \n",
    "\n",
    "        for idx in indices:\n",
    "            # Create a batch with the amount of `time_steps` images\n",
    "            X_images_batch.append(images[idx:idx + time_steps])\n",
    "\n",
    "            # Create a batch with the amount of `time_steps` exogenous factors\n",
    "            exogenous_batch.append(exo_factors[idx:idx + time_steps])\n",
    "\n",
    "            # For better prediction, we add the exogenous factors for the output\n",
    "            future_exo_batch.append(exo_factors[idx + time_steps + 1])\n",
    "\n",
    "            # The output is the image after the time steps\n",
    "            Y_image_batch.append(images[idx + time_steps])\n",
    "\n",
    "        # Convert the lists to NumPy arrays\n",
    "        X_images_batch = np.array(X_images_batch)\n",
    "        exogenous_batch = np.array(exogenous_batch)\n",
    "        future_exo_batch = np.array(future_exo_batch)\n",
    "        Y_image_batch = np.array(Y_image_batch)\n",
    "\n",
    "        # Convert the batches to TensorFlow tensors\n",
    "        yield (\n",
    "            (tf.convert_to_tensor(X_images_batch, dtype=tf.float32),\n",
    "             tf.convert_to_tensor(exogenous_batch, dtype=tf.float32),\n",
    "             tf.convert_to_tensor(future_exo_batch, dtype=tf.float32)),\n",
    "            tf.convert_to_tensor(Y_image_batch, dtype=tf.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338fec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the evaluation metric Intersection over Union (IoU) for the model\n",
    "# The IoU (Intersection over Union) metric calculates the overlap between predicted and true regions of the positive class.\n",
    "# Set a threshold for binary classification. If the value is greater than the threshold, it is classified as 1 (water), otherwise 0 (no water).\n",
    "# The threshold is set to 0.93, which is the same as the 93rd percentile of NDWI values and suitable for the project area.\n",
    "\n",
    "# IoU should be interpreted carefully in cases where the positive class occupies a small fraction of the image.\n",
    "\n",
    "def iou_metric(y_true, y_pred, threshold=0.93):\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true = tf.cast(y_true > threshold, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "\n",
    "    # Avoid division by zero\n",
    "    iou = tf.math.divide_no_nan(intersection, union)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92dde6",
   "metadata": {},
   "source": [
    "## Step 4: Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96181ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_model_with_future_exo(image_shape, exo_input_shape, future_exo_shape, time_steps):\n",
    "    # The model uses a CNN-LSTM architecture with small pooling sizes and 'valid' padding.\n",
    "    # Smaller pooling sizes help retain more spatial details, which is crucial for detecting small features like rivers.\n",
    "    # 'Valid' padding ensures no artificial padding is added, preserving the integrity of the input data.\n",
    "    # This design is tailored to the project area to improve the model's ability to predict water bodies in satellite imagery.\n",
    "    cnn_input = layers.Input(shape=(time_steps, *image_shape))  \n",
    "    x = layers.Conv3D(8, (3, 3, 3), activation='relu', padding='valid')(cnn_input)  \n",
    "    x = layers.MaxPooling3D((1, 2, 2), padding='valid')(x)  \n",
    "    x = layers.Conv3D(16, (3, 3, 3), activation='relu', padding='valid')(x)  \n",
    "    x = layers.MaxPooling3D((1, 2, 2), padding='valid')(x)  \n",
    "    x = layers.Flatten()(x)  # Flatten the output for the Dense layer\n",
    "    \n",
    "    # LSTM Layer for exogenous factors\n",
    "    exo_input = layers.Input(shape=(time_steps, exo_input_shape))  \n",
    "    exo_lstm = layers.LSTM(16)(exo_input)  \n",
    "\n",
    "    future_exo_input = layers.Input(shape=(future_exo_shape,))  \n",
    "    future_exo_dense = layers.Dense(4, activation='relu')(future_exo_input)\n",
    "\n",
    "    combined = layers.concatenate([x, exo_lstm, future_exo_dense])\n",
    "\n",
    "    # Dense Layer for final prediction of the next image\n",
    "    output = layers.Dense(image_shape[0] * image_shape[1], activation='sigmoid')(combined)\n",
    "    output = layers.Reshape((image_shape[0], image_shape[1], 1))(output)\n",
    "\n",
    "    # Define the model\n",
    "    model = models.Model(inputs=[cnn_input, exo_input, future_exo_input], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Shape of the input data must be defined\n",
    "image_shape = (49, 520, 1)  # Shape of the images e. g. (height, width, channels)\n",
    "exo_input_shape = exo_factors.shape[1]   # Amount of features in the exogenous factors\n",
    "future_exo_shape = exo_factors.shape[1]  \n",
    "time_steps = 5  \n",
    "\n",
    "# Create model\n",
    "model = build_cnn_lstm_model_with_future_exo(image_shape, exo_input_shape, future_exo_shape, time_steps)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', iou_metric])\n",
    "\n",
    "# Summarise the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbba15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train generator is a tensorflow generator that yields batches of data for training the model.\n",
    "train_generator = data_generator_with_future_exo(train_images, train_exo, batch_size=32, time_steps=time_steps)\n",
    "\n",
    "# Validation generator is a tensorflow generator that yields batches of data for validating the model.\n",
    "val_generator = data_generator_with_future_exo(val_images, val_exo, batch_size=32, time_steps=time_steps)\n",
    "\n",
    "# Train the model \n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_images) // 32,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_images) // 32  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model (architecture, weights, and training configuration)\n",
    "model.save('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da581770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and check summary especially the shape of the input layer\n",
    "model = load_model('')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d35080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sequences_with_future_exo(images_test, exo_factors_test, time_steps=5, return_y=False):\n",
    "    \"\"\"\n",
    "    Create sequences of images and exogenous factors for testing. The shape of the input data must be the same as in the training data.\n",
    "    Args:\n",
    "        images_test (np.array): Array of test images.\n",
    "        exo_factors_test (np.array): Array of test exogenous factors.\n",
    "        time_steps (int): Number of time steps for the sequence.\n",
    "        return_y (bool): Whether to return the target variable Y.\n",
    "    Returns:\n",
    "        tuple: Tuple containing the sequences of images, exogenous factors, future exogenous factors, and target variable Y (if return_y is True).  \n",
    "    \"\"\"\n",
    "    X_image_seqs = []\n",
    "    X_exo_seqs = []\n",
    "    future_exo_seqs = []\n",
    "    Y_image_seqs = []\n",
    "\n",
    "    max_start = len(images_test) - time_steps - 1  \n",
    "\n",
    "    for i in range(max_start):\n",
    "        img_seq = images_test[i:i + time_steps]  \n",
    "        exo_seq = exo_factors_test[i:i + time_steps]  \n",
    "        future_exo = exo_factors_test[i + time_steps]  \n",
    "        \n",
    "        X_image_seqs.append(img_seq)\n",
    "        X_exo_seqs.append(exo_seq)\n",
    "        future_exo_seqs.append(future_exo)\n",
    "\n",
    "\n",
    "        if return_y:\n",
    "            y_img = images_test[i + time_steps]  \n",
    "            Y_image_seqs.append(y_img)\n",
    "\n",
    "    if return_y:\n",
    "        return (\n",
    "            np.array(X_image_seqs), \n",
    "            np.array(X_exo_seqs),    \n",
    "            np.array(future_exo_seqs), \n",
    "            np.array(Y_image_seqs)   \n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            np.array(X_image_seqs), \n",
    "            np.array(X_exo_seqs), \n",
    "            np.array(future_exo_seqs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db58ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test sequences with future exogenous factors\n",
    "X_img_seq, X_exo_seq, future_exo_seq, Y_true = create_test_sequences_with_future_exo(test_images, test_exo, time_steps=5, return_y=True)\n",
    "\n",
    "# Convert the sequences to float32\n",
    "X_img_seq = X_img_seq.astype('float32')\n",
    "X_exo_seq = X_exo_seq.astype('float32')\n",
    "future_exo_seq = future_exo_seq.astype('float32')\n",
    "\n",
    "# Predict the images using the model\n",
    "predicted_images = model.predict((X_img_seq, X_exo_seq, future_exo_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc13fa0",
   "metadata": {},
   "source": [
    "## Step 5: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Heatmap of the river shift by comparing the predicted image with the last actual image\n",
    "\n",
    "last_actual_image = Y_true[i-1].squeeze()\n",
    "predicted_image = predicted_images[i].squeeze()\n",
    "\n",
    "# Calculate the difference to visualize the river shift and create a mask to highlight significant changes\n",
    "river_shift = predicted_image - last_actual_image\n",
    "alpha_mask = np.where((river_shift < -0.15) | (river_shift > 0.15), 1.0, 0.0)\n",
    "\n",
    "# Create a heatmap of the river shift\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Heatmap of River Shift\")\n",
    "plt.imshow(river_shift, cmap='coolwarm', vmin=-1, vmax=1, alpha=alpha_mask)  \n",
    "plt.colorbar(label=\"Shift Intensity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d5af9",
   "metadata": {},
   "source": [
    "### Improvement: \n",
    "AI-powered modeling is a powerful tool to address complex hydrological tasks. The notebook could be improved by using the Dice Loss function instead of binary cross-entropy or a combination of both. The Dice Loss function is more suitable for imbalanced datasets, as it focuses on the overlap between predicted and actual regions, which is critical for detecting water bodies.\n",
    "\n",
    "Additionally, visually inspecting the generated water masks and removing images with poor results could enhance the dataset quality and improve model performance. The integration of Digital Elevation Models (DEMs) to identify preferential river paths and refine water masks did not succeed in this case. However, exploring alternative methods to incorporate DEM data, such as using it as an additional input feature or applying terrain-based corrections, could still provide valuable insights and improve predictions. A further look on the application of post-processing techniques like morphological operations (e.g., dilation or erosion) could help to refine the predicted water masks and remove noise.\n",
    "\n",
    "Another key limitation is the resolution of freely available satellite imagery, which is currently limited to 10 meters for Sentinel-2. Higher-resolution imagery, if available in the future, could significantly enhance the ability to extract water bodies more accurately, especially in narrow or fragmented river systems.\n",
    "\n",
    "Further improvements could include:\n",
    "* Multi-Sensor Data Fusion: Combining data from multiple satellite sensors (e.g., Sentinel-1 SAR data for flood detection) could provide complementary information and improve water body detection under challenging conditions, such as cloud cover. Unlike optical sensors (e.g., Sentinel-2), SAR can penetrate clouds and operate in all weather conditions, making it ideal for monitoring water bodies during cloudy or rainy periods. SAR is sensitive to surface roughness and moisture, which can help distinguish between water and other land cover types, even in challenging conditions like flooded areas or wetlands.\n",
    "* Advanced Architectures: Exploring advanced deep learning architectures, such as U-Net or Transformer-based models, could improve spatial feature extraction and temporal modeling.\n",
    "* Uncertainty Quantification: Implementing methods to quantify prediction uncertainty could help identify areas where the model is less confident, guiding further data collection or model refinement.\n",
    "* Improve the implication of the Hydrological Components: Include cumulative precipitation or discharge over specific seasons (e.g., spring or summer) to capture broader hydrological trends. Add lagged versions of exogenous factors (e.g., precipitation or discharge from the previous month) to model delayed hydrological responses.\n",
    "* Explainability: Adding explainability techniques, such as saliency maps, could help understand which features or regions in the input data contribute most to the predictions, aiding model interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "river_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
