{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca0bb1c",
   "metadata": {},
   "source": [
    "# Using the Model for Prediction and Streamlit Implementation\n",
    "\n",
    "The model was trained and tested on data from **April 2017 to March 2025**. It accounts for **seasonal fluctuations** as well as complex input data such as **binary water masks with limited structural information**.\n",
    "\n",
    "To make a prediction, the method `model.predict()` must be used. It is important to ensure that the input data (e. g. preprocessed satellite images and exogenous factors) **match the structure and dimensions** used during training.\n",
    "\n",
    "### Requirements for Predictions\n",
    "\n",
    "- **Satellite Images**  \n",
    "  At least the **last 5 images** are required.\n",
    "\n",
    "- **Exogenous Factors**  \n",
    "  Relevant data such as **precipitation** and **discharge**, which can be obtained from:\n",
    "  - Local authorities (e. g. *Gewässerkundlicher Dienst Bayern*)\n",
    "  - Web scraping sources (e. g. **OpenMeteo**). Note: Data sources may not always be consistent or fully aligned.\n",
    "\n",
    "### Implementation in Streamlit\n",
    "\n",
    "To integrate the model into **Streamlit**, a `.py` script is required that includes the described workflow.\n",
    "\n",
    "A **customizable template** will be provided to support this implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "import ee\n",
    "import geemap\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rasterio\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import folium\n",
    "from folium.raster_layers import ImageOverlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c751fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Get Satellite Imagery from Google Earth Engine\n",
    "\n",
    "\n",
    "# Initialize Earth Engine API\n",
    "# Make sure to authenticate if you haven't done so already\n",
    "#ee.Authenticate()  # Uncomment this line if you need to authenticate\n",
    "# Initialize the Earth Engine library\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "\n",
    "#Define the coordinates of the bounding box (xmin, ymin) and (xmax, ymax) of your project area\n",
    "#Coordinates are in EPSG:4326\n",
    "#Example coordinates for a bounding box around Vorderriß, Germany\n",
    "bbox_coords = ((11.437948183612978, 47.55891195678332), (11.484581611702358, 47.56320074444652))\n",
    "geometry = ee.Geometry.Polygon([\n",
    "    [\n",
    "        [bbox_coords[0][0], bbox_coords[0][1]],  # (xmin, ymin)\n",
    "        [bbox_coords[0][0], bbox_coords[1][1]],  # (xmin, ymax)\n",
    "        [bbox_coords[1][0], bbox_coords[1][1]],  # (xmax, ymax)\n",
    "        [bbox_coords[1][0], bbox_coords[0][1]],  # (xmax, ymin)\n",
    "        [bbox_coords[0][0], bbox_coords[0][1]],  # Closing the polygon\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Define a function to calculate snow fraction of each image. Snow has similiar reflectance properties as water.\n",
    "def add_snow_fraction(image):\n",
    "    scl = image.select(\"SCL\")\n",
    "    snow_mask = scl.eq(11)  # SCL = 11 means snow/ice\n",
    "\n",
    "    snow_fraction = snow_mask.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=geometry,\n",
    "        scale=20,\n",
    "        maxPixels=1e8\n",
    "    ).get('SCL')  # result is a number between 0 and 1\n",
    "\n",
    "    return image.set('snow_fraction', snow_fraction)\n",
    "\n",
    "\n",
    "# Define NDWI calculation function (normalized difference using B3 and B8)\n",
    "def calculateNDWI(image):\n",
    "    ndwi = image.normalizedDifference([\"B3\", \"B8\"]).rename(\"NDWI\")\n",
    "    return image.addBands(ndwi)\n",
    "\n",
    "# Define a function to clip the image to your geometry\n",
    "def clip_image(image):\n",
    "    return image.clip(geometry)\n",
    "\n",
    "# Create a function to assign a chronological image_id based on the collection index\n",
    "def create_feature_with_index(image, index):\n",
    "    # Convert the index to a string to be used as the image ID\n",
    "    image_id = ee.Number(index)  # Using index as the image ID\n",
    "    timestamp = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')  # Format timestamp as YYYY-MM-DD\n",
    "    \n",
    "    # Create a feature with the image ID and timestamp\n",
    "    feature = ee.Feature(None, {\n",
    "        'image_id': image_id,\n",
    "        'timestamp': timestamp\n",
    "    })\n",
    "    return feature\n",
    "\n",
    "# Function to assign a chronological index to each image in the collection\n",
    "def add_index_to_collection(image_collection):\n",
    "    # Create a list of features with image_id as index\n",
    "    def assign_index(image, index):\n",
    "        return create_feature_with_index(image, index)\n",
    "    \n",
    "    # Map the function over the collection and add indices\n",
    "    feature_collection = image_collection.map(lambda image: assign_index(image, image_collection.toList(image_collection.size()).indexOf(image)))\n",
    "    return feature_collection\n",
    "\n",
    "\n",
    "# Step 1. 1: Load Sentinel-2 image collection. Filter by date, bounds, and cloud cover.\n",
    "sentinel = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "    .filterDate('2015-06-27', '2025-03-31') \\\n",
    "    .filterBounds(geometry) \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))  \n",
    "\n",
    "# Add snow fraction to each image\n",
    "sentinel_snow_filtered = sentinel.map(add_snow_fraction)\n",
    "\n",
    "# Filter images with < 3 % snow\n",
    "sentinel_clean = sentinel_snow_filtered.filter(ee.Filter.lt('snow_fraction', 0.03))\n",
    "\n",
    "processed_images = sentinel_clean.map(clip_image).map(calculateNDWI)\n",
    "\n",
    "\n",
    "# Map the function over the processed image collection to create a FeatureCollection with chronological image_ids\n",
    "features = add_index_to_collection(processed_images)\n",
    "\n",
    "# Step 1. 2: Export images and metadata to Google Drive. If you authenticated with your Google account in Google Earth Engine, the images will be saved in your Google Drive.\n",
    "\n",
    "# Export the FeatureCollection as a CSV to Google Drive\n",
    "export_task = ee.batch.Export.table.toDrive(\n",
    "    collection=features,\n",
    "    description='Image_Metadata',\n",
    "    fileFormat='CSV',\n",
    "    folder='Satellite_metadata',  # Folder in your Google Drive\n",
    "    fileNamePrefix='satellite_metadata_all',\n",
    ")\n",
    "\n",
    "# Start the export task\n",
    "export_task.start()\n",
    "\n",
    "print(\"Export task for image metadata started. Monitor Earth Engine for task completion.\")\n",
    "\n",
    "\n",
    "# Export each image in the collection\n",
    "def export_image(image, idx):  \n",
    "    # Export the image to a GeoTIFF using Earth Engine's export method\n",
    "    export_task = ee.batch.Export.image.toDrive(\n",
    "        image=image.select(\"NDWI\"),\n",
    "        description=None,\n",
    "        folder=\"GEE_Images_all\",\n",
    "        fileNamePrefix=f\"NDWI_{idx}\",\n",
    "        region=geometry,\n",
    "        scale=10,\n",
    "        crs='EPSG:4326',\n",
    "        maxPixels=1e8  # Increase the max pixels if necessary\n",
    "    )\n",
    "    export_task.start()  # Start the export task\n",
    "    print(f\"Exporting {idx}\")\n",
    "\n",
    "# Convert the image collection to a list of images. This triggers the actual fetching of the collection.\n",
    "image_list = processed_images.toList(processed_images.size())\n",
    "\n",
    "# Loop through the image collection and export each image\n",
    "for idx in range(image_list.size().getInfo()):\n",
    "    ee_image = ee.Image(image_list.get(idx))  # Get the image by its index in the list\n",
    "    export_image(ee_image, idx)\n",
    "\n",
    "print(\"Export tasks started. Monitor Earth Engine for task completion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 3: Add an index column to the CSV file exported from Earth Engine\n",
    "\n",
    "metadata_path = ''  \n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "metadata_df['Index'] = range(len(metadata_df)) #Creating an index column with ascending values\n",
    "\n",
    "output_path = ''  # Save the modified DataFrame to a new CSV file\n",
    "metadata_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Added new column 'Index' and saved it in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c195575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the ID from the filename\n",
    "# This function assumes the filename format is \"NDWI_<ID>.tif\"\n",
    "def extract_id_from_filename(filename):\n",
    "    return int(filename.split('_')[1].split('.')[0]) \n",
    "\n",
    "# Define a function to get the timestamp from the image ID and double check if the ID is in the new CSV file \n",
    "def get_timestamp_from_id(image_id):\n",
    "    timestamp_data = pd.read_csv('')  \n",
    "    row = timestamp_data[timestamp_data['Index'] == image_id]\n",
    "    if not row.empty:\n",
    "        return row['timestamp'].values[0]\n",
    "    else:\n",
    "        return None  # If the ID is not found, return None\n",
    "\n",
    "# Define a function to process NDWI images and create water masks\n",
    "def process_images_and_create_water_masks(folder_path):\n",
    "    output_dir_water_masks = '/'  # folder for saving water masks\n",
    "\n",
    "    # Make sure that the folder exists\n",
    "    if not os.path.exists(output_dir_water_masks):\n",
    "        os.makedirs(output_dir_water_masks)\n",
    "\n",
    "    # Order by numeric ID\n",
    "    image_files =[f for f in os.listdir(folder_path) if f.lower().endswith(('.tif', '.tiff'))]\n",
    "    image_files.sort(key=extract_id_from_filename)  \n",
    "\n",
    "    # Loop through each image file\n",
    "    for filename in image_files:\n",
    "        if filename.lower().endswith(('.tif', '.tiff')):\n",
    "            image_id = extract_id_from_filename(filename)\n",
    "            timestamp = get_timestamp_from_id(image_id)\n",
    "            if timestamp is None:\n",
    "                print(f\"No timestamp for {filename}. Pass.\")\n",
    "                continue\n",
    "            \n",
    "            # Open the NDWI image using GDAL\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            dataset = gdal.Open(file_path)\n",
    "            if dataset is None:\n",
    "                print(f\"There was a mistake while opening: {filename}.\")\n",
    "                continue\n",
    "            \n",
    "            # Read the NDWI band and calculate the individual threshold. \n",
    "            # For the project area possible fraction of water in the image is between 5,8 % and 6,5 %. \n",
    "            # Assuming a unique threshold for each image is not recommended.\n",
    "            band = dataset.GetRasterBand(1)\n",
    "            ndwi = band.ReadAsArray()\n",
    "            \n",
    "            threshold = np.percentile(ndwi, 93)\n",
    "            if np.isnan(threshold):\n",
    "                threshold = 0.05\n",
    "                print(f\"Threshold for {filename} is NaN. Set to {threshold}.\")\n",
    "            # Check if the threshold is within the expected range. Unless change it.\n",
    "            elif threshold < -0.1 or threshold > 0.1:\n",
    "                # Get the sign of the threshold (-1, +1)\n",
    "                sign = np.sign(threshold) \n",
    "                \n",
    "                # If the threshold is negative, set it to -0.1, but proportional to the size of the threshold\n",
    "                if threshold < -0.1:\n",
    "                    threshold = -0.1 + (threshold + 0.1) * 0.7\n",
    "                # If the threshold is positive, set it to 0.1, but proportional to the size of the threshold\n",
    "                elif threshold > 0.1:\n",
    "                    threshold = 0.1 - (threshold - 0.1) * 0.7\n",
    "\n",
    "                threshold = np.clip(threshold, -0.1, 0.1)\n",
    "                \n",
    "                print(f\"Invalid threshold for {filename}: Set to {threshold}.\")\n",
    "            else:\n",
    "                print(f\"Valid threshold for {filename} is {threshold}.\")\n",
    "\n",
    "            # Create a binary water mask based on the threshold and save it as a new GeoTIFF \n",
    "            water_mask = ndwi > threshold\n",
    "        \n",
    "            water_mask_filename = os.path.join(output_dir_water_masks, f\"water_mask_{os.path.splitext(filename)[0]}.tif\")\n",
    "            driver = gdal.GetDriverByName('GTiff')\n",
    "            mask_dataset = driver.Create(\n",
    "                water_mask_filename,\n",
    "                dataset.RasterXSize,\n",
    "                dataset.RasterYSize,\n",
    "                1,\n",
    "                gdal.GDT_Byte\n",
    "            )\n",
    "            mask_dataset.SetGeoTransform(dataset.GetGeoTransform())\n",
    "            mask_dataset.SetProjection(dataset.GetProjection())\n",
    "            mask_band = mask_dataset.GetRasterBand(1)\n",
    "            mask_band.WriteArray(water_mask.astype(np.uint8)) \n",
    "            mask_dataset = None\n",
    "            print(f\"Saved water mask: {water_mask_filename}\")\n",
    "\n",
    "    print(\"All water masks created and saved.\")\n",
    "    return\n",
    "\n",
    "\n",
    "folder_path = ''  # Path to the folder with NDWI images\n",
    "\n",
    "# Call the function to process images and create water masks\n",
    "process_images_and_create_water_masks(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. 1: Load water masks from the folder and add a new dimension to the images because the model requires 3D input or 4D input\n",
    "\n",
    "def extract_id_from_filename(filename):\n",
    "    return int(filename.split('_')[-1].split('.')[0]) \n",
    "\n",
    "# Define a function to load all TIF images from a directory in sorted order\n",
    "def load_all_tif_images(directory):\n",
    "    image_files = [f for f in os.listdir(directory) if f.lower().endswith('.tif')]\n",
    "    image_files.sort(key=extract_id_from_filename)\n",
    "    \n",
    "    images = []\n",
    "    for filename in image_files:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            image = src.read(1)  \n",
    "            image = np.expand_dims(image, axis=-1) # Add a new dimension\n",
    "            images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "images_directory = \"/\" # Path to the folder with water masks\n",
    "images = load_all_tif_images(images_directory)\n",
    "\n",
    "print(f\"Amount of loaded images: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c072679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3. 2: Prepare exogenous factors for data set\n",
    "\n",
    "exo_factors_df = pd.read_csv(\"\")  # Path to the CSV file with exogenous factors\n",
    "\n",
    "# Convert the 'timestamp' column to datetime format\n",
    "exo_factors_df['timestamp'] = pd.to_datetime(exo_factors_df['timestamp'])\n",
    "\n",
    "# Calculate the time difference in days between consecutive timestamps\n",
    "exo_factors_df['time_diff_days'] = exo_factors_df['timestamp'].diff().dt.days\n",
    "\n",
    "# Extract the month from the timestamp and one-hot encode it and save the columns names\n",
    "exo_factors_df['month'] = exo_factors_df['timestamp'].dt.month\n",
    "month_one_hot = pd.get_dummies(exo_factors_df['month'], prefix='month')\n",
    "month_one_hot_columns = list(month_one_hot.columns)\n",
    "\n",
    "# Add the one-hot encoded month columns to the DataFrame\n",
    "exo_factors_df = pd.concat([exo_factors_df, month_one_hot], axis=1)\n",
    "\n",
    "# Drop the original month column\n",
    "exo_factors_df.drop(columns=['month'], inplace=True)\n",
    "\n",
    "# Convert other important columns of the DataFrame. Save only the columns that are needed for the model.\n",
    "exo_factors_df['discharge'] = pd.to_numeric(exo_factors_df['delta_discharge'], errors='coerce')\n",
    "exo_factors_df['precipitation'] = pd.to_numeric(exo_factors_df['delta_precipitation'], errors='coerce')\n",
    "exo_factors_df['prec_extreme'] = pd.to_numeric(exo_factors_df['extreme_precipitation'], errors='coerce')\n",
    "exo_factors_df['disc_extreme'] = pd.to_numeric(exo_factors_df['extreme_discharge'], errors='coerce')\n",
    "\n",
    "exo_factors_df = exo_factors_df[['timestamp', 'image_id', 'discharge', 'precipitation', 'prec_extreme', 'disc_extreme', 'time_diff_days'] + list(month_one_hot.columns)]\n",
    "exo_factors_df.fillna(0, inplace=True)\n",
    "print(exo_factors_df.head())\n",
    "\n",
    "# Convert the DataFrame to a NumPy array because the model requires 3D input or 4D input\n",
    "exo_factors = exo_factors_df[['discharge', 'precipitation', 'prec_extreme', 'disc_extreme', 'time_diff_days'] + list(month_one_hot_columns)].values\n",
    "print(exo_factors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and check summary especially the shape of the input layer\n",
    "model = load_model('')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70498de",
   "metadata": {},
   "source": [
    "Future exogenous values required for forecasting can be manually added—either by editing the CSV file or appending to the input array—or automatically calculated based on user inputs provided through the Streamlit interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4727b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sequences_with_future_exo(images, exo_factors, time_steps=5):\n",
    "    \"\"\"\n",
    "    Create sequences of images and exogenous factors for testing. The shape of the input data must be the same as in the training data.\n",
    "    Args:\n",
    "        images (np.array): Array of images.\n",
    "        exo_factors_test (np.array): Array of exogenous factors.\n",
    "        time_steps (int): Number of time steps for the sequence.\n",
    "        return_y (bool): Whether to return the target variable Y.\n",
    "    Returns:\n",
    "        tuple: Tuple containing the sequences of images, exogenous factors, future exogenous factors, and target variable Y (if return_y is True).  \n",
    "    \"\"\"\n",
    "    X_image_seqs = []\n",
    "    X_exo_seqs = []\n",
    "    future_exo_seqs = []\n",
    "\n",
    "    max_start = len(images) - time_steps - 1  \n",
    "\n",
    "    for i in range(max_start):\n",
    "        img_seq = images[i:i + time_steps]  \n",
    "        exo_seq = exo_factors[i:i + time_steps]  \n",
    "        future_exo = exo_factors[i + time_steps]  \n",
    "        \n",
    "        X_image_seqs.append(img_seq)\n",
    "        X_exo_seqs.append(exo_seq)\n",
    "        future_exo_seqs.append(future_exo)\n",
    "\n",
    "        return (\n",
    "            np.array(X_image_seqs), \n",
    "            np.array(X_exo_seqs), \n",
    "            np.array(future_exo_seqs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences with future exogenous factors\n",
    "X_img_seq, X_exo_seq, future_exo_seq = create_test_sequences_with_future_exo(images, exo_factors, time_steps=5)\n",
    "\n",
    "# Convert the sequences to float32\n",
    "X_img_seq = X_img_seq.astype('float32')\n",
    "X_exo_seq = X_exo_seq.astype('float32')\n",
    "future_exo_seq = future_exo_seq.astype('float32')\n",
    "\n",
    "# Predict the image(s) using the model\n",
    "predicted_images = model.predict((X_img_seq, X_exo_seq, future_exo_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Heatmap of the river shift by comparing the predicted image with the last actual image\n",
    "# Depending on the amount of satellite images you were providing to the model, you can change the index of the last actual image\n",
    "# index of the last predicted image is: len(images) - time_steps - 1\n",
    "\n",
    "last_actual_image = images[i-time_steps-1].squeeze()\n",
    "predicted_image = predicted_images[i].squeeze()\n",
    "\n",
    "# Calculate the difference to visualize the river shift and create a mask to highlight significant changes\n",
    "river_shift = predicted_image - last_actual_image\n",
    "alpha_mask = np.where((river_shift < -0.15) | (river_shift > 0.15), 1.0, 0.0)\n",
    "\n",
    "# Create a heatmap of the river shift\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Heatmap of River Shift\")\n",
    "plt.imshow(river_shift, cmap='coolwarm', vmin=-1, vmax=1, alpha=alpha_mask)  \n",
    "plt.colorbar(label=\"Shift Intensity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the river shift in the right bounds therefore everything additional must be removed\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(river_shift, cmap='coolwarm', vmin=-1, vmax=1, alpha=alpha_mask)  \n",
    "plt.axis(\"off\")\n",
    "plt.gca().set_position([0,0,1,1])\n",
    "plt.savefig(\"river_shift_overlay.png\", dpi=300, bbox_inches=\"tight\", pad_inches=0, transparent=True)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box for the image\n",
    "bounds = [[47.55891195678332, 11.437948183612978], [47.56320074444652, 11.484581611702358]]\n",
    "\n",
    "m = folium.Map(location=[47.561, 11.461], zoom_start=14)\n",
    "\n",
    "# Add the base layer (OpenStreetMap) and the overlay \n",
    "ImageOverlay(\n",
    "    image=\"rivershift_overlay.png\",  \n",
    "    bounds=bounds,\n",
    "    opacity=0.6,  \n",
    "    interactive=True\n",
    ").add_to(m)\n",
    "\n",
    "# Add Layer Control in order to switch between layers\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m.save(\".html\")\n",
    "print(\"Map saved as '.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64150e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show map in VS Code\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "river_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
